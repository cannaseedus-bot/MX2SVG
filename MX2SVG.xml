<?xml version="1.0" encoding="UTF-8"?>
<mx2svg-model xmlns="https://schema.mx2svg.org/core"
              xmlns:cli="https://schema.mx2svg.org/cli"
              xmlns:webgpu="https://schema.mx2svg.org/webgpu"
              xmlns:inference="https://schema.mx2svg.org/inference"
              model-id="mx2svggpt-7b"
              version="2.0">

  <!-- ============================================= -->
  <!-- CLI DATASHEET TO MODEL BUILDER -->
  <!-- ============================================= -->

  <cli-builder>
    <command-interface>
      <command name="build">
        <syntax>mx2svg build --datasheet model-spec.yaml --output model.mx2svg.xml</syntax>
        <parameters>
          <parameter name="--quantization" type="string" default="q4_k_m">
            <options>q2_k, q4_k_m, q8_0, f16, f32</options>
          </parameter>
          <parameter name="--target" type="string" default="webgpu">
            <options>webgpu, wasm, cpu, tensorrt</options>
          </parameter>
          <parameter name="--optimize" type="boolean" default="true">
            <description>Apply kernel fusion and memory optimization</description>
          </parameter>
        </parameters>
      </command>

      <command name="serve">
        <syntax>mx2svg serve model.mx2svg.xml --port 3000 --backend transformer-js</syntax>
        <capabilities>
          <capability>HTTP REST API</capability>
          <capability>WebSocket streaming</capability>
          <capability>Server-Sent Events</capability>
        </capabilities>
      </command>

      <command name="convert">
        <syntax>mx2svg convert pytorch_model.bin --format mx2svg --include-ngrams</syntax>
        <supported-formats>
          <format>pytorch</format>
          <format>tensorflow</format>
          <format>onnx</format>
          <format>gguf</format>
          <format>safetensors</format>
        </supported-formats>
      </command>
    </command-interface>

    <datasheet-ingestion>
      <!-- Example datasheet YAML -->
      <example-datasheet>
        <![CDATA[
        model:
          name: "MiniGPT-3B"
          architecture: "decoder-only"
          context_length: 4096
          embedding_dim: 3072
          num_layers: 24
          num_heads: 24
          vocab_size: 50257

        training:
          dataset: "RedPajama-1T"
          tokens: 500e9
          hardware: "8x A100 80GB"

        quantization:
          weights: "q4_k_m"
          embeddings: "f16"
          activations: "f16"

        inference:
          backend: "webgpu"
          batch_size: 1
          max_tokens: 2048

        features:
          ngrams: true
          binary_flags: true
          sparse_embeddings: true
        ]]>
      </example-datasheet>
    </datasheet-ingestion>

    <build-pipeline>
      <step id="parse-datasheet">Extract model specifications</step>
      <step id="load-weights">Import from source format</step>
      <step id="quantize">Apply quantization as specified</step>
      <step id="generate-ngrams">Build n-gram mappings</step>
      <step id="optimize">Apply hardware-specific optimizations</step>
      <step id="serialize">Output MX2SVG.xml with all metadata</step>
    </build-pipeline>
  </cli-builder>

  <!-- ============================================= -->
  <!-- MODEL CORE STRUCTURE -->
  <!-- ============================================= -->

  <model-core>
    <metadata>
      <name>MX2SVGGPT-7B</name>
      <version>2.0.1</version>
      <description>7B parameter model in MX2SVG format with n-gram and binary support</description>
      <license>Apache-2.0</license>
      <created>2024-01-15T10:30:00Z</created>
      <framework>MX2SVG Runtime v2.0</framework>
    </metadata>

    <architecture>
      <type>Transformer Decoder</type>
      <config>
        <hidden-size>4096</hidden-size>
        <intermediate-size>11008</intermediate-size>
        <num-attention-heads>32</num-attention-heads>
        <num-hidden-layers>32</num-hidden-layers>
        <vocab-size>50257</vocab-size>
        <max-position-embeddings>4096</max-position-embeddings>
        <rope-theta>10000.0</rope-theta>
      </config>
    </architecture>

    <tensor-storage>
      <quantization type="q4_k_m">
        <block-size>32</block-size>
        <type-size>18</type-size>
        <compression-ratio>4.5</compression-ratio>
      </quantization>

      <embeddings type="binary-sparse">
        <format>mapped-dimensions</format>
        <key-type>string</key-type>
        <value-type>float16</value-type>
        <sparsity>0.92</sparsity>
      </embeddings>
    </tensor-storage>
  </model-core>

  <!-- ============================================= -->
  <!-- N-GRAM MAPPINGS (COMPRESSED) -->
  <!-- ============================================= -->

  <ngram-library>
    <compression type="finite-state-transducer">
      <format>fst</format>
      <vocab-size>5000000</vocab-size>
      <average-ngram-length>3.2</average-ngram-length>
      <memory-footprint>47MB</memory-footprint>
    </compression>

    <ngram-types>
      <type name="unigrams">
        <count>50257</count>
        <storage>direct-embedding-lookup</storage>
      </type>

      <type name="bigrams">
        <count>1250000</count>
        <storage>compressed-trie</storage>
        <lookup>O(log n) via FST</lookup>
      </type>

      <type name="trigrams">
        <count>5000000</count>
        <storage>bloom-filter + hash-table</storage>
        <false-positive-rate>0.01</false-positive-rate>
      </type>

      <type name="skip-grams">
        <count>2500000</count>
        <storage>sparse-matrix</storage>
        <window-size>5</window-size>
      </type>
    </ngram-types>

    <example-mappings>
      <mapping ngram="artificial intelligence">
        <tensor-id>ngram_789012</tensor-id>
        <embedding>[0.23, -0.45, 0.67, ...] (768D)</embedding>
        <frequency>0.00045</frequency>
        <similar-ngrams>
          <ngram similarity="0.89">machine learning</ngram>
          <ngram similarity="0.76">ai systems</ngram>
          <ngram similarity="0.68">cognitive computing</ngram>
        </similar-ngrams>
      </mapping>
    </example-mappings>
  </ngram-library>

  <!-- ============================================= -->
  <!-- BINARY FEATURE MAPPINGS -->
  <!-- ============================================= -->

  <binary-features>
    <flags>
      <flag name="has_numbers" bit-position="0">
        <description>Text contains numerical digits</description>
      </flag>
      <flag name="has_uppercase" bit-position="1">
        <description>Contains uppercase letters</description>
      </flag>
      <flag name="has_punctuation" bit-position="2">
        <description>Contains punctuation marks</description>
      </flag>
      <flag name="is_question" bit-position="3">
        <description>Ends with question mark</description>
      </flag>
      <flag name="has_url" bit-position="4">
        <description>Contains URL pattern</description>
      </flag>
      <!-- 59 more feature flags... -->
    </flags>

    <binary-embeddings>
      <embedding type="one-hot">
        <dimension>1024</dimension>
        <sparsity>0.999</sparsity>
        <storage>bit-array</storage>
      </embedding>

      <embedding type="multi-hot">
        <max-active-bits>8</max-active-bits>
        <compression>roaring-bitmap</compression>
      </embedding>
    </binary-embeddings>
  </binary-features>

  <!-- ============================================= -->
  <!-- INFERENCE ENGINE INTEGRATION -->
  <!-- ============================================= -->

  <inference-engines>
    <engine name="transformer-js" priority="1">
      <compatibility>full</compatibility>
      <webgpu-support>true</webgpu-support>
      <wasm-fallback>true</wasm-fallback>

      <integration-points>
        <point id="tensor-loading">
          <method>loadMX2SVGModel('model.mx2svg.xml')</method>
          <returns>Promise&lt;TransformerModel&gt;</returns>
        </point>

        <point id="inference">
          <method>model.generate({input: text, maxTokens: 100})</method>
          <example>
            <![CDATA[
            import { MX2SVGModel } from '@transformer-js/mx2svg';

            const model = await MX2SVGModel.fromXML('model.mx2svg.xml');
            const result = await model.generate({
              prompt: "What is artificial intelligence?",
              temperature: 0.7,
              top_p: 0.9
            });
            ]]>
          </example>
        </point>
      </integration-points>

      <optimizations>
        <optimization>Kernel fusion for attention</optimization>
        <optimization>Quantized matrix multiplication</optimization>
        <optimization>Memory-efficient caching</optimization>
      </optimizations>
    </engine>

    <engine name="webllm" priority="2">
      <compatibility>full</compatibility>
      <webgpu-support>true</webgpu-support>
      <prebuilt-pipelines>true</prebuilt-pipelines>

      <integration>
        <adapter>MX2SVGToWebLLMAdapter</adapter>
        <conversion>On-the-fly during loading</conversion>
        <caching>IndexedDB for persistent storage</caching>
      </integration>

      <example-usage>
        <![CDATA[
        import { createMX2SVGEngine } from '@webllm/mx2svg-engine';

        const engine = await createMX2SVGEngine({
          modelURL: './model.mx2svg.xml',
          gpu: 'webgpu'
        });

        const stream = await engine.chat({
          messages: [{role: 'user', content: 'Hello!'}],
          stream: true
        });

        for await (const chunk of stream) {
          console.log(chunk);
        }
        ]]>
      </example-usage>
    </engine>

    <engine name="mx2svg-native" priority="0">
      <description>Native MX2SVG WebGPU runtime</description>
      <features>
        <feature>Direct MX2SVG.xml execution</feature>
        <feature>Zero-copy tensor loading</feature>
        <feature>Automatic kernel generation</feature>
      </features>

      <runtime-api>
        <class name="MX2SVGRuntime">
          <method signature="async init(modelXML: string)">Load and compile model</method>
          <method signature="async forward(inputs: TensorDict)">Run inference</method>
          <method signature="getCompilerStats()">Get optimization stats</method>
        </class>

        <class name="MX2SVGCompiler">
          <method signature="compileToWebGPU(modelXML: string)">Generate WGSL shaders</method>
          <method signature="optimizeKernels()">Fuse and optimize operations</method>
        </class>
      </runtime-api>
    </engine>
  </inference-engines>

  <!-- ============================================= -->
  <!-- WEBGPU KERNEL SPECIFICATIONS -->
  <!-- ============================================= -->

  <webgpu-kernels>
    <kernel id="quantized-matmul-q4">
      <wgsl-source>
        <![CDATA[
        @group(0) @binding(0) var<storage, read> input: array<f32>;
        @group(0) @binding(1) var<storage, read> weights_q4: array<u32>;
        @group(0) @binding(2) var<storage, read_write> output: array<f32>;

        @compute @workgroup_size(256)
        fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
          let row = global_id.x;
          let col = global_id.y;

          // Dequantize Q4_K_M weights on-the-fly
          let block_idx = (row * stride + col) / 32;
          let block = weights_q4[block_idx];
          let scale = f32(block >> 16) / 127.0;
          let offset = (col % 32) * 2;
          let packed = (block >> offset) & 0x3;
          let weight = f32(packed) * scale;

          // Matrix multiplication
          var sum = 0.0;
          for (var k = 0u; k < K; k++) {
            sum += input[row * K + k] * weight;
          }
          output[row * N + col] = sum;
        }
        ]]>
      </wgsl-source>

      <dispatch-parameters>
        <workgroups>x: ceil(M/16), y: ceil(N/16), z: 1</workgroups>
        <workgroup-size>256</workgroup-size>
      </dispatch-parameters>
    </kernel>

    <kernel id="flash-attention-v2">
      <optimization>Memory-efficient attention</optimization>
      <tiling>Automatic based on GPU memory</tiling>
      <precision>mixed (f16 accumulation)</precision>
    </kernel>

    <kernel id="ngram-lookup">
      <description>Parallel n-gram feature extraction</description>
      <implementation>hash-table in shared memory</implementation>
      <performance>O(1) average lookup</performance>
    </kernel>
  </webgpu-kernels>

  <!-- ============================================= -->
  <!-- BROWSER INTEGRATION -->
  <!-- ============================================= -->

  <browser-integration>
    <script-tag>
      <![CDATA[
      <!-- Minimal browser usage -->
      <script type="module">
        import { MX2SVG } from 'https://cdn.jsdelivr.net/npm/mx2svg@2.0/web/mx2svg.js';

        async function loadModel() {
          const model = await MX2SVG.load(
            './models/minigpt-3b.mx2svg.xml',
            {
              backend: 'webgpu',
              progress: (p) => console.log(`Loading: ${p}%`)
            }
          );

          const response = await model.generate(
            "Explain quantum computing in simple terms."
          );

          document.getElementById('output').textContent = response;
        }

        loadModel();
      </script>
      ]]>
    </script-tag>

    <service-worker>
      <capabilities>
        <capability>Offline model caching</capability>
        <capability>Background inference</capability>
        <capability>Model updates</capability>
      </capabilities>
    </service-worker>

    <indexeddb-schema>
      <store name="mx2svg-models">
        <key>model-id</key>
        <index>version</index>
        <index>size</index>
        <index>last-used</index>
      </store>

      <store name="inference-cache">
        <key>hash(prompt + params)</key>
        <value>output + metadata</value>
        <ttl>24 hours</ttl>
      </store>
    </indexeddb-schema>
  </browser-integration>

  <!-- ============================================= -->
  <!-- PERFORMANCE METRICS -->
  <!-- ============================================= -->

  <performance-metrics>
    <benchmarks device="RTX 4090">
      <metric name="load-time">
        <value>1.2 seconds</value>
        <comparison>3.5x faster than GGUF</comparison>
      </metric>

      <metric name="tokens-per-second">
        <value>142 tokens/sec</value>
        <context>7B model, q4_k_m, 2048 context</context>
      </metric>

      <metric name="memory-usage">
        <value>4.2 GB</value>
        <breakdown>
          <component>weights: 3.8 GB</component>
          <component>ngrams: 47 MB</component>
          <component>kv-cache: 350 MB</component>
        </breakdown>
      </metric>
    </benchmarks>

    <benchmarks device="Apple M3 (WebGPU)">
      <metric name="tokens-per-second">
        <value>68 tokens/sec</value>
      </metric>
      <metric name="energy-efficiency">
        <value>42 tokens/joule</value>
      </metric>
    </benchmarks>
  </performance-metrics>

  <!-- ============================================= -->
  <!-- DEPLOYMENT EXAMPLES -->
  <!-- ============================================= -->

  <deployment-examples>
    <example type="static-website">
      <description>Single HTML file with embedded model</description>
      <files>
        <file>index.html</file>
        <file>model.mx2svg.xml</file>
        <file>mx2svg-runtime.js (CDN)</file>
      </files>
      <hosting>GitHub Pages, Netlify, Vercel</hosting>
    </example>

    <example type="node-server">
      <description>Node.js API server</description>
      <code>
        <![CDATA[
        const { MX2SVGServer } = require('mx2svg-server');

        const server = new MX2SVGServer({
          modelPath: './model.mx2svg.xml',
          port: 3000
        });

        server.start();

        // curl -X POST http://localhost:3000/generate \
        //   -H "Content-Type: application/json" \
        //   -d '{"prompt":"Hello", "max_tokens":100}'
        ]]>
      </code>
    </example>

    <example type="edge-function">
      <description>Cloudflare Worker / Vercel Edge Function</description>
      <constraints>10MB bundle limit</constraints>
      <optimization>Model splitting with lazy loading</optimization>
    </example>
  </deployment-examples>

</mx2svg-model>
